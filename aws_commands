aws configure ### to login to aws cmd line but it ask for the aws access key and secret##

aws s3api create-bucket --bucket ANYBUCKETNAME --region us-east-1   ##ANYBUCKETNAME is the unique name we need to provide###

aws s3 ls --region us-east-1     ## to list buckets ##

aws s3 cp examplefile  s3://ANYBUCKETNAME/examplefile  ## to copy file from cmdline

aws s3 ls s3://ANYBUCKETNAME  --region us-east-1       ## to list the files from the cmd line

aws s3 rm s3://ANYBUCKETNAME/examplefile               ## to delete the file from the cmd line



################# ECR ###################

#aws ecr create-repository --repository-name hello-repository --region us-east-1

output:
REPOSITORY      2024-03-08T12:52:23.208000-05:00        MUTABLE 381492****    arn:aws:ecr:us-east-1:3814922****:repository/hello-repository  hello-repository      3814******.dkr.ecr.us-east-1.amazonaws.com/hello-repository

#docker tag hello-world:latest 3814922****.dkr.ecr.us-east-1.amazonaws.com/hello-repository

#docker images
REPOSITORY                                                      TAG       IMAGE ID       CREATED         SIZE
381492*****.dkr.ecr.us-east-1.amazonaws.com/hello-repository   latest    a81f976411f8   4 minutes ago   370MB

###LOGIN TO ECR#####

#aws2 ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 3814******.dkr.ecr.us-east-1.amazonaws.com
Login Succeeded

#docker push 3814922******.dkr.ecr.us-east-1.amazonaws.com/hello-repository
Using default tag: latest
The push refers to repository [381492*****.dkr.ecr.us-east-1.amazonaws.com/hello-repository]
bce7691983bf: Pushed 
8f28be65afdd: Pushed 
815b6ea5484b: Pushed 
2ceae5655886: Pushed 
latest: digest: sha256:ea2af498b04e5e85cfeab9db9b418009112ed89d609c05645ebe6e79a77105b8 size: 1155

##delete images in ECR ###
#aws ecr batch-delete-image --repository-name hello-repository --image-ids imageTag=latest --region us-east-1
IMAGEIDS        sha256:ea2af498b04e5e85cfeab9db9b418009112ed89d609c05645ebe6e79a77105b8 latest

##delete repositroy###
aws2 ecr delete-repository --repository-name hello-repository --force --region us-east-1
REPOSITORY      2024-03-08T12:52:23.208000-05:00        MUTABLE 381492240550    arn:aws:ecr:us-east-1:381492****:repository/hello-repository  hello-repository      3814922****.dkr.ecr.us-east-1.amazonaws.com/hello-repository


___________
#aws2 ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 381492*****.dkr.ecr.us-east-1.amazonaws.com/demorepo

#docker tag hello-world:latest 381492******.dkr.ecr.us-east-1.amazonaws.com/demorepo

#docker images      
3814922*****.dkr.ecr.us-east-1.amazonaws.com/demorepo           latest    a81f976411f8   27 minutes ago   370MB

#docker push 381492*****.dkr.ecr.us-east-1.amazonaws.com/demorepo
Using default tag: latest
The push refers to repository [381492*****.dkr.ecr.us-east-1.amazonaws.com/demorepo]
bce7691983bf: Pushed 
8f28be65afdd: Pushed 
815b6ea5484b: Pushed 
2ceae5655886: Pushed 
latest: digest: sha256:ea2af498b04e5e85cfeab9db9b418009112ed89d609c05645ebe6e79a77105b8 size: 1155

SEE images in README file how it looks on aws console



####EKS clusters from command lines #####

We need to install EKSCTL before we execute this command

#eksctl create cluster --name qa-cluster --version 1.27 --region us-east-1 --nodegroup-name linux-nodes --node-type t2.micro --nodes 2

sample output-
eksctl version 0.175.0
2024-04-02 12:44:40 [ℹ]  using region us-east-1
2024-04-02 12:44:40 [ℹ]  setting availability zones to [us-east-1c us-east-1b]
2024-04-02 12:44:40 [ℹ]  subnets for us-east-1c - public:192.168.0.0/19 private:192.168.64.0/19
2024-04-02 12:44:40 [ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.96.0/19
2024-04-02 12:44:40 [ℹ]  nodegroup "linux-nodes" will use "" [AmazonLinux2/1.27]
2024-04-02 12:44:40 [ℹ]  using Kubernetes version 1.27
2024-04-02 12:44:40 [ℹ]  creating EKS cluster "qa-cluster" in "us-east-1" region with managed nodes
2024-04-02 12:44:40 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2024-04-02 12:44:40 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=qa-cluster'
2024-04-02 12:44:40 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "qa-cluster" in "us-east-1"
2024-04-02 12:44:40 [ℹ]  CloudWatch logging will not be enabled for cluster "qa-cluster" in "us-east-1"
2024-04-02 12:44:40 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=qa-cluster'
2024-04-02 12:44:40 [ℹ]  
2 sequential tasks: { create cluster control plane "qa-cluster", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup "linux-nodes",
    } 
}
2024-04-02 12:44:40 [ℹ]  building cluster stack "eksctl-qa-cluster-cluster"
2024-04-02 12:44:41 [ℹ]  deploying stack "eksctl-qa-cluster-cluster"
2024-04-02 12:45:11 [ℹ]  waiting for CloudFormation stack "eksctl-qa-cluster-cluster"

#kubectl get nodes

#kubectl get ns

#kubectl get cs

#eksctl delete cluster --name qa-cluster --region us-east-1 
2024-04-02 13:10:44 [ℹ]  deleting EKS cluster "qa-cluster"
2024-04-02 13:10:45 [ℹ]  will drain 0 unmanaged nodegroup(s) in cluster "qa-cluster"
2024-04-02 13:10:45 [ℹ]  starting parallel draining, max in-flight of 1
2024-04-02 13:10:45 [ℹ]  deleted 0 Fargate profile(s)
2024-04-02 13:10:46 [✔]  kubeconfig has been updated
2024-04-02 13:10:46 [ℹ]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress
2024-04-02 13:10:47 [ℹ]  
2 sequential tasks: { delete nodegroup "linux-nodes", delete cluster control plane "qa-cluster" [async] 








